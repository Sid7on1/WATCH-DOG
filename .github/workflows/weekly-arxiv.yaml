name: ðŸš€ Weekly ArXiv Paper Implementation Generator

on:
  schedule:
    # Run every Sunday at 2:00 AM UTC (adjust for your timezone)
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      max_papers:
        description: 'Maximum number of papers to process'
        required: false
        default: '3'
        type: string
      specific_domain:
        description: 'Focus on specific research domain (e.g., nlp, cv, ml)'
        required: false
        type: string
      dry_run:
        description: 'Run in dry-run mode (no repos created)'
        required: false
        default: false
        type: boolean
      force_reprocess:
        description: 'Force reprocess existing papers'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  MAX_PAPERS_DEFAULT: 3
  PROCESSING_TIMEOUT: 180  # 3 hours max

jobs:
  # Job 1: Fetch and prepare papers
  fetch-papers:
    name: ðŸ“š Fetch ArXiv Papers
    runs-on: ubuntu-latest
    outputs:
      papers-found: ${{ steps.count-papers.outputs.count }}
      should-process: ${{ steps.check-papers.outputs.should-process }}
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.API_GITHUB }}
        fetch-depth: 0
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ðŸ” Validate Environment
      run: |
        echo "ðŸ”‘ Checking required secrets..."
        if [ -z "${{ secrets.OPENROUTER_API_KEY }}" ]; then
          echo "::error::OPENROUTER_API_KEY secret is required"
          exit 1
        fi
        if [ -z "${{ secrets.API_GITHUB }}" ]; then
          echo "::error::API_GITHUB secret is required"
          exit 1
        fi
        echo "âœ… All required secrets are present"
    
    - name: ðŸ“š Fetch and Process ArXiv Papers
      run: |
        echo "ðŸš€ Running advanced ArXiv pipeline..."
        
        # Set parameters based on inputs
        DOMAINS="${{ github.event.inputs.specific_domain || 'cs.AI cs.LG cs.CV cs.CL' }}"
        MAX_PAPERS="${{ github.event.inputs.max_papers || '15' }}"
        
        # Run the complete pipeline
        python advanced_paper_extractor.py \
          --arxiv \
          --domains $DOMAINS \
          --max-papers $MAX_PAPERS \
          --delay 8 \
          --threshold 0.7 \
          --verbose
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.API_GITHUB }}
        GITHUB_USERNAME: "Sid7on1"
        USERNAME_GITHUB: "Sid7on1"
    
    - name: ðŸ“Š Count Available Papers
      id: count-papers
      run: |
        pdf_count=0
        json_count=0
        
        if [ -d "relevant_pdfs" ]; then
          pdf_count=$(find relevant_pdfs -name "*.pdf" -type f | wc -l)
        fi
        
        if [ -d "relevant_json" ]; then
          json_count=$(find relevant_json -name "*.json" -type f | wc -l)
        fi
        
        total_count=$((pdf_count + json_count))
        
        echo "count=$total_count" >> $GITHUB_OUTPUT
        echo "pdf_count=$pdf_count" >> $GITHUB_OUTPUT
        echo "json_count=$json_count" >> $GITHUB_OUTPUT
        
        echo "ðŸ“Š Papers found:"
        echo "   PDFs: $pdf_count"
        echo "   JSONs: $json_count"
        echo "   Total: $total_count"
    
    - name: âœ… Check Processing Conditions
      id: check-papers
      run: |
        should_process="false"
        
        # Check if we have papers to process
        if [ "${{ steps.count-papers.outputs.count }}" -gt "0" ]; then
          should_process="true"
        fi
        
        # Always process on manual trigger
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          should_process="true"
        fi
        
        echo "should-process=$should_process" >> $GITHUB_OUTPUT
        echo "ðŸŽ¯ Should process papers: $should_process"
    
    - name: ðŸ“¤ Upload Papers for Processing
      if: steps.check-papers.outputs.should-process == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: papers-to-process
        path: |
          relevant_pdfs/
          relevant_json/
        retention-days: 1

  # Job 2: Generate repositories (simplified - no separate extraction)
  generate-repositories:
    name: ðŸ—ï¸ Generate GitHub Repositories
    runs-on: ubuntu-latest
    needs: fetch-papers
    if: needs.fetch-papers.outputs.should-process == 'true'
    timeout-minutes: ${{ fromJson(env.PROCESSING_TIMEOUT) }}
    
    strategy:
      fail-fast: false
      matrix:
        batch: [1, 2]
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.API_GITHUB }}
    
    - name: ðŸ Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ðŸ“¦ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ðŸ“¥ Download Papers
      uses: actions/download-artifact@v4
      with:
        name: papers-to-process
        path: .
    
    - name: âš™ï¸ Configure Git
      run: |
        git config --global user.name "M1-Evo-Agent[bot]"
        git config --global user.email "m1-evo-agent@users.noreply.github.com"
    
    - name: ðŸ—ï¸ Generate Repositories with Dual Saving
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.API_GITHUB }}
        GITHUB_USERNAME: "Sid7on1"
        GITHUB_ACTIONS: "true"
        BATCH_NUMBER: ${{ matrix.batch }}
        DRY_RUN: ${{ github.event.inputs.dry_run }}
        MAX_PAPERS: ${{ github.event.inputs.max_papers || env.MAX_PAPERS_DEFAULT }}
      run: |
        echo "ðŸ—ï¸ Starting dual repository generation (Batch ${{ matrix.batch }})"
        echo "âœ… Individual repos + ðŸ’¾ WATCHDOG_memory backups"
        
        # Create necessary directories
        mkdir -p workspace logs llm_interactions cache
        
        # Build command arguments
        cmd_args="--verbose"
        
        if [ "$DRY_RUN" = "true" ]; then
          cmd_args="$cmd_args --dry-run"
          echo "ðŸ” Running in DRY RUN mode - no repositories will be created"
        fi
        
        # Set max papers per batch
        max_papers=${MAX_PAPERS:-3}
        papers_per_batch=$((max_papers / 2 + 1))
        cmd_args="$cmd_args --max-papers $papers_per_batch"
        
        echo "ðŸš€ Running M1-Evo Agent with args: $cmd_args"
        echo "ðŸ“Š Processing up to $papers_per_batch papers in batch ${{ matrix.batch }}"
        
        # Run the main agent (creates individual repos + WATCHDOG_memory backups)
        python fix2.py $cmd_args
    
    - name: ðŸ“Š Generate Batch Report
      if: always()
      run: |
        echo "## ðŸ—ï¸ Dual Repository Generation Report (Batch ${{ matrix.batch }})" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "managed_repos_state.json" ]; then
          echo "### ï¿½ Pcrocessing Results:" >> $GITHUB_STEP_SUMMARY
          python -c "
import json
import sys
try:
    with open('managed_repos_state.json', 'r') as f:
        data = json.load(f)
    
    total = len(data)
    success = sum(1 for repo in data.values() if repo.get('status') == 'success')
    failed = sum(1 for repo in data.values() if repo.get('status') == 'failed')
    
    print(f'- **Total repositories processed:** {total}')
    print(f'- **Individual repos created:** {success}')
    print(f'- **WATCHDOG_memory backups:** {success}')
    print(f'- **Failed:** {failed}')
    print('')
    
    if success > 0:
        print('### âœ… Successfully Created (Individual + WATCHDOG_memory):')
        for name, repo in data.items():
            if repo.get('status') == 'success':
                url = repo.get('github_url', 'N/A')
                print(f'- **Individual**: [{name}]({url})')
                print(f'- **WATCHDOG Backup**: \`projects/{name}/\`')
        print('')
    
    if failed > 0:
        print('### âŒ Failed Repositories:')
        for name, repo in data.items():
            if repo.get('status') == 'failed':
                errors = repo.get('errors', [])
                error_msg = errors[-1] if errors else 'Unknown error'
                print(f'- **{name}**: {error_msg}')
        print('')
        
except Exception as e:
    print(f'Error reading state file: {e}')
" >> $GITHUB_STEP_SUMMARY
        else
          echo "No repositories were processed in this batch." >> $GITHUB_STEP_SUMMARY
        fi
        
        # Add processing stats
        echo "### ðŸ“ˆ Processing Statistics:" >> $GITHUB_STEP_SUMMARY
        echo "- **Batch Number:** ${{ matrix.batch }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Dual Saving:** Individual repos + WATCHDOG_memory backups" >> $GITHUB_STEP_SUMMARY
        echo "- **Dry Run Mode:** ${{ github.event.inputs.dry_run || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Processing Time:** $(date -u)" >> $GITHUB_STEP_SUMMARY
    
    - name: ðŸ“¤ Upload Generation Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: generation-results-batch-${{ matrix.batch }}
        path: |
          workspace/
          logs/
          llm_interactions/
          managed_repos_state.json
        retention-days: 30

  # Job 3: Consolidate and report
  consolidate-results:
    name: ðŸ“‹ Consolidate Results
    runs-on: ubuntu-latest
    needs: [fetch-papers, generate-repositories]
    if: always() && needs.fetch-papers.outputs.should-process == 'true'
    
    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
    
    - name: ðŸ“¥ Download All Results
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: ðŸ“Š Generate Consolidated Report
      run: |
        echo "# ðŸš€ Weekly ArXiv Paper Implementation Report" > WEEKLY_REPORT.md
        echo "" >> WEEKLY_REPORT.md
        echo "**Generated:** $(date -u)" >> WEEKLY_REPORT.md
        echo "**Trigger:** ${{ github.event_name }}" >> WEEKLY_REPORT.md
        echo "**Papers Found:** ${{ needs.fetch-papers.outputs.papers-found }}" >> WEEKLY_REPORT.md
        echo "**Dual Saving:** Individual repositories + WATCHDOG_memory backups" >> WEEKLY_REPORT.md
        echo "" >> WEEKLY_REPORT.md
        
        # Consolidate results from all batches
        total_repos=0
        total_success=0
        total_failed=0
        
        for batch_dir in artifacts/generation-results-batch-*/; do
          if [ -f "$batch_dir/managed_repos_state.json" ]; then
            batch_num=$(echo "$batch_dir" | grep -o 'batch-[0-9]*' | cut -d'-' -f2)
            echo "## ðŸ“¦ Batch $batch_num Results" >> WEEKLY_REPORT.md
            
            python3 -c "
import json
import sys
import os
try:
    with open('$batch_dir/managed_repos_state.json', 'r') as f:
        data = json.load(f)
    
    batch_total = len(data)
    batch_success = sum(1 for repo in data.values() if repo.get('status') == 'success')
    batch_failed = sum(1 for repo in data.values() if repo.get('status') == 'failed')
    
    print(f'- Repositories processed: {batch_total}')
    print(f'- Individual repos created: {batch_success}')
    print(f'- WATCHDOG_memory backups: {batch_success}')
    print(f'- Failed: {batch_failed}')
    print('')
    
    # Save successful repos
    for name, repo in data.items():
        if repo.get('status') == 'success':
            url = repo.get('github_url', 'N/A')
            with open('successful_repos.txt', 'a') as f:
                f.write(f'{name}|{url}\n')
    
    # Save failed repos
    for name, repo in data.items():
        if repo.get('status') == 'failed':
            errors = repo.get('errors', [])
            error_msg = errors[-1] if errors else 'Unknown error'
            with open('failed_repos.txt', 'a') as f:
                f.write(f'{name}|{error_msg}\n')
    
    # Output stats for shell
    print(f'BATCH_TOTAL={batch_total}', file=sys.stderr)
    print(f'BATCH_SUCCESS={batch_success}', file=sys.stderr)
    print(f'BATCH_FAILED={batch_failed}', file=sys.stderr)
    
except Exception as e:
    print(f'Error processing batch: {e}')
" 2>batch_stats.txt >> WEEKLY_REPORT.md
            
            # Read batch stats
            if [ -f "batch_stats.txt" ]; then
              source batch_stats.txt 2>/dev/null || true
              total_repos=$((total_repos + ${BATCH_TOTAL:-0}))
              total_success=$((total_success + ${BATCH_SUCCESS:-0}))
              total_failed=$((total_failed + ${BATCH_FAILED:-0}))
              rm -f batch_stats.txt
            fi
          fi
        done
        
        # Overall summary
        echo "" >> WEEKLY_REPORT.md
        echo "## ðŸŽ¯ Overall Summary" >> WEEKLY_REPORT.md
        echo "- **Total Repositories:** $total_repos" >> WEEKLY_REPORT.md
        echo "- **Individual Repos Created:** $total_success" >> WEEKLY_REPORT.md
        echo "- **WATCHDOG_memory Backups:** $total_success" >> WEEKLY_REPORT.md
        echo "- **Failed:** $total_failed" >> WEEKLY_REPORT.md
        
        if [ $total_repos -gt 0 ]; then
          success_rate=$((total_success * 100 / total_repos))
          echo "- **Success Rate:** ${success_rate}%" >> WEEKLY_REPORT.md
        fi
        
        # List successful repositories
        if [ -f "successful_repos.txt" ] && [ -s "successful_repos.txt" ]; then
          echo "" >> WEEKLY_REPORT.md
          echo "## âœ… Successfully Created:" >> WEEKLY_REPORT.md
          echo "" >> WEEKLY_REPORT.md
          while IFS='|' read -r name url; do
            echo "### ðŸ“ $name" >> WEEKLY_REPORT.md
            echo "- **Individual Repository**: [$name]($url)" >> WEEKLY_REPORT.md
            echo "- **WATCHDOG_memory Backup**: \`projects/$name/\`" >> WEEKLY_REPORT.md
            echo "" >> WEEKLY_REPORT.md
          done < successful_repos.txt
        fi
        
        # List failed repositories
        if [ -f "failed_repos.txt" ] && [ -s "failed_repos.txt" ]; then
          echo "" >> WEEKLY_REPORT.md
          echo "## âŒ Failed Repositories:" >> WEEKLY_REPORT.md
          while IFS='|' read -r name error; do
            echo "- **$name**: $error" >> WEEKLY_REPORT.md
          done < failed_repos.txt
        fi
        
        echo "" >> WEEKLY_REPORT.md
        echo "---" >> WEEKLY_REPORT.md
        echo "*Generated by M1-Evo Maintainer Agent with Dual Saving*" >> WEEKLY_REPORT.md
    
    - name: ðŸ“¤ Upload Final Report
      uses: actions/upload-artifact@v4
      with:
        name: weekly-implementation-report
        path: WEEKLY_REPORT.md
        retention-days: 90
    
    - name: ðŸ“Š Display Summary
      run: |
        echo "## ðŸŽ‰ Weekly ArXiv Processing Complete!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        cat WEEKLY_REPORT.md >> $GITHUB_STEP_SUMMARY

  # Job 4: Cleanup old artifacts
  cleanup:
    name: ðŸ§¹ Cleanup
    runs-on: ubuntu-latest
    needs: [consolidate-results]
    if: always()
    
    steps:
    - name: ðŸ§¹ Clean up old artifacts
      uses: actions/github-script@v7
      with:
        script: |
          // Clean up artifacts older than 30 days
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
          });
          
          const thirtyDaysAgo = new Date();
          thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);
          
          let deletedCount = 0;
          for (const artifact of artifacts.data.artifacts) {
            const createdAt = new Date(artifact.created_at);
            if (createdAt < thirtyDaysAgo && 
                (artifact.name.includes('papers-to-process') || 
                 artifact.name.includes('extracted-content') || 
                 artifact.name.includes('generation-results'))) {
              console.log(`Deleting old artifact: ${artifact.name}`);
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
              deletedCount++;
            }
          }
          
          console.log(`Cleaned up ${deletedCount} old artifacts`);
          
          // Set output for summary
          core.summary.addHeading('ðŸ§¹ Cleanup Complete');
          core.summary.addRaw(`Deleted ${deletedCount} old artifacts`);
          await core.summary.write();
