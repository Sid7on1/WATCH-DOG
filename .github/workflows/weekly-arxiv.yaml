name: üöÄ Daily ArXiv AI/ML Paper Implementation Generator

on:
  schedule:
    - cron: '0 3 * * *'  # Every night at 3:00 AM UTC
  workflow_dispatch:
    inputs:
      max_papers:
        description: 'Maximum number of papers to process'
        required: false
        default: '5'
        type: string
      dry_run:
        description: 'Run in dry-run mode (no repos created)'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  process-papers:
    name: üöÄ Process ArXiv Papers
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
    - name: üì• Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.API_GITHUB }}
        fetch-depth: 0
    
    - name: üêç Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: üì¶ Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: ÔøΩ Validate Eonvironment
      run: |
        echo "Checking required secrets..."
        if [ -z "${{ secrets.OPENROUTER_API_KEY }}" ]; then
          echo "::error::OPENROUTER_API_KEY secret is required"
          exit 1
        fi
        if [ -z "${{ secrets.API_GITHUB }}" ]; then
          echo "::error::API_GITHUB secret is required"
          exit 1
        fi
        echo "All required secrets are present"
    
    - name: ‚öôÔ∏è Configure Git
      run: |
        git config --global user.name "M1-Evo-Agent[bot]"
        git config --global user.email "m1-evo-agent@users.noreply.github.com"
    
    - name: üìö Fetch and Extract Papers from ArXiv
      run: |
        echo "Running ArXiv paper extraction pipeline..."
        
        # Comprehensive AI/ML/DL domains - automatically included every night
        # cs.AI: Artificial Intelligence
        # cs.LG: Machine Learning  
        # cs.CV: Computer Vision
        # cs.CL: Natural Language Processing
        # cs.NE: Neural Networks
        # cs.RO: Robotics
        # cs.HC: Human-Computer Interaction
        # stat.ML: Statistics - Machine Learning
        # cs.IR: Information Retrieval
        # cs.MM: Multimedia
        # cs.SD: Sound Processing
        # cs.DC: Distributed Computing
        # cs.CR: Cryptography and Security
        # cs.IT: Information Theory
        DOMAINS="cs.AI cs.LG cs.CV cs.CL cs.NE cs.RO cs.HC stat.ML cs.IR cs.MM cs.SD cs.DC cs.CR cs.IT"
        MAX_PAPERS="${{ github.event.inputs.max_papers || '5' }}"
        
        python advanced_paper_extractor.py \
          --arxiv \
          --domains $DOMAINS \
          --max-papers $MAX_PAPERS \
          --delay 8 \
          --threshold 0.7 \
          --verbose
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.API_GITHUB }}
        GITHUB_USERNAME: "Sid7on1"
    
    - name: üèóÔ∏è Generate Repositories with Dual Saving
      run: |
        echo "Generating repositories with dual saving..."
        echo "Individual repos + WATCHDOG_memory backups"
        
        CMD_ARGS="--verbose"
        
        if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
          CMD_ARGS="$CMD_ARGS --dry-run"
          echo "Running in DRY RUN mode"
        fi
        
        MAX_PAPERS="${{ github.event.inputs.max_papers || '3' }}"
        CMD_ARGS="$CMD_ARGS --max-papers $MAX_PAPERS"
        
        echo "Running: python fix2.py $CMD_ARGS"
        python fix2.py $CMD_ARGS
      env:
        OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        GITHUB_TOKEN: ${{ secrets.API_GITHUB }}
        GITHUB_USERNAME: "Sid7on1"
        GITHUB_ACTIONS: "true"
    
    - name: üìä Upload Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: processing-results
        path: |
          logs/
          workspace/
          managed_repos_state.json
          relevant_json/
          relevant_pdfs/
        retention-days: 30
    
    - name: üìã Generate Summary Report
      if: always()
      run: |
        echo "Generating execution summary..."
        
        if [ -f "managed_repos_state.json" ]; then
          echo "Repository processing completed"
          echo "State file found: managed_repos_state.json"
        else
          echo "No state file found - check logs for details"
        fi
        
        echo "Processing completed at: $(date -u)"
    
    - name: üßπ Cleanup Temporary Files
      if: always()
      run: |
        echo "Cleaning up temporary files..."
        rm -rf extraction_cache/ || true
        rm -rf __pycache__/ || true
        echo "Cleanup completed"

  cleanup-artifacts:
    name: üßπ Cleanup Old Artifacts
    runs-on: ubuntu-latest
    needs: [process-papers]
    if: always()
    
    steps:
    - name: üßπ Clean up old artifacts
      uses: actions/github-script@v7
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
          });
          
          const thirtyDaysAgo = new Date();
          thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30);
          
          let deletedCount = 0;
          for (const artifact of artifacts.data.artifacts) {
            const createdAt = new Date(artifact.created_at);
            if (createdAt < thirtyDaysAgo) {
              console.log(`Deleting old artifact: ${artifact.name}`);
              await github.rest.actions.deleteArtifact({
                owner: context.repo.owner,
                repo: context.repo.repo,
                artifact_id: artifact.id,
              });
              deletedCount++;
            }
          }
          
          console.log(`Cleaned up ${deletedCount} old artifacts`);
