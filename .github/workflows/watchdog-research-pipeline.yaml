name: 🔬 WATCHDOG Full Research Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily research pipeline at 1:20 AM India time (19:50 UTC)
    - cron: '50 19 * * *'
  workflow_dispatch:
    inputs:
      max_papers_per_domain:
        description: 'Maximum papers to scrape per domain'
        required: false
        default: '5'
        type: string
      skip_scraping:
        description: 'Skip paper scraping (use existing PDFs)'
        required: false
        default: false
        type: boolean
      skip_extraction:
        description: 'Skip PDF text extraction'
        required: false
        default: false
        type: boolean
      skip_selection:
        description: 'Skip relevance selection'
        required: false
        default: false
        type: boolean
      skip_planning:
        description: 'Skip project planning'
        required: false
        default: false
        type: boolean
      skip_coding:
        description: 'Skip code generation'
        required: false
        default: false
        type: boolean
      force_deploy:
        description: 'Force deployment to GitHub'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  MAX_PAPERS_PER_DOMAIN: ${{ github.event.inputs.max_papers_per_domain || '5' }}

jobs:
  # ============================================================================
  # STAGE 1: PAPER SCRAPING
  # ============================================================================
  paper-scraping:
    name: 📚 ArXiv Paper Scraping
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_scraping != 'true'
    outputs:
      papers_scraped: ${{ steps.scrape-papers.outputs.papers_count }}
      scraping_success: ${{ steps.scrape-papers.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install WATCHDOG Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for scraping
          pip install requests python-dotenv lxml beautifulsoup4 arxiv urllib3
        fi
        echo "✅ Dependencies installed for scraping stage"
    
    - name: 🔐 Configure API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "🔑 Configuring API keys for WATCHDOG system..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "GEMINI_API_KEY=$GEMINI_API_KEY" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "✅ Environment configured with all API keys"
    
    - name: 📚 Run ArXiv Paper Scraper
      id: scrape-papers
      run: |
        echo "🔍 Starting ArXiv paper scraping..."
        echo "📊 Max papers per domain: ${{ env.MAX_PAPERS_PER_DOMAIN }}"
        
        python -c "
        import sys
        import traceback
        
        try:
            import scraper
            print('✅ Scraper module loaded successfully')
            
            # Initialize scraper
            arxiv_scraper = scraper.ArxivScraper()
            print('✅ ArXiv scraper initialized')
            
            # Run scraping with specified limits
            max_papers = int('${{ env.MAX_PAPERS_PER_DOMAIN }}')
            results = arxiv_scraper.scrape_all_domains(
                max_results_per_domain=max_papers,
                download_pdfs=True
            )
            
            # Count total papers
            total_papers = sum(len(papers) for papers in results.values())
            print(f'📊 Total papers scraped: {total_papers}')
            
            # Save metadata and summary
            arxiv_scraper.save_consolidated_metadata(results)
            arxiv_scraper.generate_summary_report(results)
            
            # Complete workflow
            arxiv_scraper.github_manager.workflow_end()
            
            print(f'papers_count={total_papers}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('✅ Paper scraping completed successfully!')
            
        except Exception as e:
            print(f'❌ Scraping failed: {e}')
            traceback.print_exc()
            print('papers_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 📊 Upload Scraping Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: scraped-papers-${{ github.run_number }}
        path: |
          artifacts/pdfs/
          artifacts/metadata.txt
          artifacts/scraping_summary.txt
        retention-days: 7

  # ============================================================================
  # STAGE 2: PDF TEXT EXTRACTION
  # ============================================================================
  text-extraction:
    name: 📄 PDF Text Extraction
    runs-on: ubuntu-latest
    needs: paper-scraping
    if: |
      always() && 
      (needs.paper-scraping.result == 'success' || github.event.inputs.skip_scraping == 'true') &&
      github.event.inputs.skip_extraction != 'true'
    outputs:
      chunks_created: ${{ steps.extract-text.outputs.chunks_count }}
      extraction_success: ${{ steps.extract-text.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Scraped Papers
      if: needs.paper-scraping.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: scraped-papers-${{ github.run_number }}
        path: ./
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install PDF Processing Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for PDF processing
          pip install PyPDF2 pdfplumber pathlib
        fi
        echo "✅ Dependencies installed for PDF extraction stage"
    
    - name: 📄 Extract Text from PDFs
      id: extract-text
      run: |
        echo "📄 Starting PDF text extraction..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import extractor
            print('✅ Extractor module loaded successfully')
            
            # Initialize extractor
            pdf_extractor = extractor.PDFTextExtractor()
            print('✅ PDF extractor initialized')
            
            # Process all PDFs
            pdf_extractor.process_all_pdfs()
            
            # Count created chunks
            import os
            from pathlib import Path
            
            texts_dir = Path('artifacts/pdf-txts')
            chunk_count = 0
            if texts_dir.exists():
                for pdf_dir in texts_dir.iterdir():
                    if pdf_dir.is_dir():
                        chunk_files = list(pdf_dir.glob('chunk_*.txt'))
                        chunk_count += len(chunk_files)
            
            print(f'📊 Total text chunks created: {chunk_count}')
            print(f'chunks_count={chunk_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('✅ Text extraction completed successfully!')
            
        except Exception as e:
            print(f'❌ Text extraction failed: {e}')
            traceback.print_exc()
            print('chunks_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 📊 Upload Text Extraction Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: extracted-texts-${{ github.run_number }}
        path: |
          artifacts/pdf-txts/
        retention-days: 7

  # ============================================================================
  # STAGE 3: RELEVANCE SELECTION
  # ============================================================================
  relevance-selection:
    name: 🎯 Intelligent Relevance Selection
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction]
    if: |
      always() && 
      (needs.text-extraction.result == 'success' || github.event.inputs.skip_extraction == 'true') &&
      github.event.inputs.skip_selection != 'true'
    outputs:
      relevant_papers: ${{ steps.select-relevant.outputs.relevant_count }}
      selection_success: ${{ steps.select-relevant.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Extracted Texts
      if: needs.text-extraction.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: extracted-texts-${{ github.run_number }}
        path: ./
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Selection Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for AI selection
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "✅ Dependencies installed for selection stage"
    
    - name: 🔐 Configure AI API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "🔑 Configuring AI API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "✅ AI APIs configured"
    
    - name: 🎯 Run Intelligent PDF Selector
      id: select-relevant
      run: |
        echo "🎯 Starting intelligent relevance selection..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import selector
            print('✅ Selector module loaded successfully')
            
            # Initialize selector
            pdf_selector = selector.IntelligentPDFSelector()
            print('✅ Intelligent PDF selector initialized')
            
            # Process all PDF texts for relevance
            pdf_selector.process_all_pdfs()
            
            # Count relevant papers
            import os
            from pathlib import Path
            
            relevant_dir = Path('artifacts/relevant')
            relevant_count = 0
            if relevant_dir.exists():
                relevant_count = len([d for d in relevant_dir.iterdir() if d.is_dir()])
            
            print(f'📊 Relevant papers found: {relevant_count}')
            print(f'relevant_count={relevant_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('✅ Relevance selection completed successfully!')
            
        except Exception as e:
            print(f'❌ Relevance selection failed: {e}')
            traceback.print_exc()
            print('relevant_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 📊 Upload Selection Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: relevant-papers-${{ github.run_number }}
        path: |
          artifacts/relevant/
        retention-days: 7

  # ============================================================================
  # STAGE 4: PROJECT PLANNING
  # ============================================================================
  project-planning:
    name: 📋 Intelligent Project Planning
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection]
    if: |
      always() && 
      (needs.relevance-selection.result == 'success' || github.event.inputs.skip_selection == 'true') &&
      github.event.inputs.skip_planning != 'true'
    outputs:
      plans_created: ${{ steps.create-plans.outputs.plans_count }}
      planning_success: ${{ steps.create-plans.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Relevant Papers
      if: needs.relevance-selection.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: relevant-papers-${{ github.run_number }}
        path: ./
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Planning Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for planning
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "✅ Dependencies installed for planning stage"
    
    - name: 🔐 Configure Planning API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "🔑 Configuring planning API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "✅ Planning APIs configured"
    
    - name: 📋 Run Intelligent Project Planner
      id: create-plans
      run: |
        echo "📋 Starting intelligent project planning..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import planner
            print('✅ Planner module loaded successfully')
            
            # Initialize planner
            project_planner = planner.IntelligentProjectPlanner()
            print('✅ Intelligent project planner initialized')
            
            # Process all relevant papers and create plans
            project_planner.process_all_relevant_papers()
            
            # Count created plans
            import os
            from pathlib import Path
            
            structures_dir = Path('artifacts/structures')
            plans_count = 0
            if structures_dir.exists():
                for paper_dir in structures_dir.iterdir():
                    if paper_dir.is_dir() and (paper_dir / 'plan.json').exists():
                        plans_count += 1
            
            print(f'📊 Project plans created: {plans_count}')
            print(f'plans_count={plans_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('✅ Project planning completed successfully!')
            
        except Exception as e:
            print(f'❌ Project planning failed: {e}')
            traceback.print_exc()
            print('plans_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 📊 Upload Planning Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: project-plans-${{ github.run_number }}
        path: |
          artifacts/structures/
        retention-days: 7

  # ============================================================================
  # STAGE 5: PROJECT MANAGEMENT & CODE GENERATION
  # ============================================================================
  code-generation:
    name: 👥 Multi-Agent Code Generation
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning]
    if: |
      always() && 
      (needs.project-planning.result == 'success' || github.event.inputs.skip_planning == 'true') &&
      github.event.inputs.skip_coding != 'true'
    outputs:
      projects_created: ${{ steps.generate-code.outputs.projects_count }}
      coding_success: ${{ steps.generate-code.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Project Plans
      if: needs.project-planning.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: project-plans-${{ github.run_number }}
        path: ./
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Coding Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for coding (threading/queue are built-in)
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "✅ Dependencies installed for coding stage"
    
    - name: 🔐 Configure Coding API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "🔑 Configuring coding API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "✅ Coding APIs configured"
    
    - name: 👥 Run Multi-Agent Project Manager
      id: generate-code
      run: |
        echo "👥 Starting multi-agent code generation..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import manager
            print('✅ Manager module loaded successfully')
            
            # Initialize project manager
            project_manager = manager.ProjectManager()
            print('✅ Multi-agent project manager initialized')
            
            # Process all project plans and generate code
            project_manager.process_all_project_plans()
            
            # Count created projects
            import os
            from pathlib import Path
            
            projects_dir = Path('artifacts/projects')
            projects_count = 0
            if projects_dir.exists():
                projects_count = len([d for d in projects_dir.iterdir() if d.is_dir()])
            
            print(f'📊 Projects created: {projects_count}')
            print(f'projects_count={projects_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('✅ Code generation completed successfully!')
            
        except Exception as e:
            print(f'❌ Code generation failed: {e}')
            traceback.print_exc()
            print('projects_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 📊 Upload Generated Projects
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: generated-projects-${{ github.run_number }}
        path: |
          artifacts/projects/
        retention-days: 30

  # ============================================================================
  # STAGE 6: GITHUB DEPLOYMENT
  # ============================================================================
  github-deployment:
    name: 🚀 GitHub Repository Deployment
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning, code-generation]
    if: |
      always() && 
      (needs.code-generation.result == 'success' || github.event.inputs.skip_coding == 'true') &&
      github.event.inputs.force_deploy == 'true'
    outputs:
      repos_created: ${{ steps.deploy-repos.outputs.repos_count }}
      deployment_success: ${{ steps.deploy-repos.outputs.success }}
    
    steps:
    - name: 🔄 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download Generated Projects
      if: needs.code-generation.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: generated-projects-${{ github.run_number }}
        path: ./
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Deployment Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for deployment (base64/shutil/tempfile are built-in)
          pip install requests python-dotenv
        fi
        echo "✅ Dependencies installed for deployment stage"
    
    - name: 🔐 Configure GitHub API
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
      run: |
        echo "🔑 Configuring GitHub API..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "✅ GitHub API configured"
    
    - name: 🚀 Deploy Projects to GitHub Repositories
      id: deploy-repos
      run: |
        echo "🚀 Starting GitHub repository deployment..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import pusher
            print('✅ Pusher module loaded successfully')
            
            # Initialize GitHub repository manager
            github_manager = pusher.GitHubRepositoryManager()
            print('✅ GitHub repository manager initialized')
            
            # Print current status
            github_manager.print_status()
            
            # Process all projects and create repositories
            success = github_manager.process_all_projects()
            
            if success:
                print('✅ All projects deployed to GitHub successfully!')
                
                # Get repository count
                from pathlib import Path
                projects_dir = Path('artifacts/projects')
                repos_count = 0
                if projects_dir.exists():
                    repos_count = len([d for d in projects_dir.iterdir() if d.is_dir()])
                
                print(f'📊 Repositories created: {repos_count}')
                print(f'repos_count={repos_count}' >> '$GITHUB_OUTPUT')
                print('success=true' >> '$GITHUB_OUTPUT')
                
                # Final status report
                print('\\n📊 Final deployment status:')
                github_manager.print_status()
            else:
                print('❌ Some projects failed to deploy')
                print('repos_count=0' >> '$GITHUB_OUTPUT')
                print('success=false' >> '$GITHUB_OUTPUT')
                sys.exit(1)
                
        except Exception as e:
            print(f'❌ GitHub deployment failed: {e}')
            traceback.print_exc()
            print('repos_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: 🔍 Verify Repository Deployments
      env:
        GITHUB_API: ${{ secrets.API_GITHUB }}
      run: |
        echo "🔍 Verifying deployed repositories..."
        
        python -c "
        import requests
        import os
        from datetime import datetime, timedelta
        
        github_token = os.getenv('GITHUB_API')
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # Get user info
        user_response = requests.get('https://api.github.com/user', headers=headers)
        if user_response.status_code == 200:
            username = user_response.json()['login']
            print(f'👤 Verifying repositories for: {username}')
            
            # Get recent repositories
            repos_response = requests.get(
                f'https://api.github.com/users/{username}/repos?sort=created&direction=desc&per_page=20',
                headers=headers
            )
            
            if repos_response.status_code == 200:
                repos = repos_response.json()
                recent_repos = []
                
                # Filter repositories created in the last 2 hours
                cutoff_time = datetime.now() - timedelta(hours=2)
                
                for repo in repos:
                    created_at = datetime.fromisoformat(repo['created_at'].replace('Z', '+00:00'))
                    if created_at.replace(tzinfo=None) >= cutoff_time:
                        recent_repos.append(repo)
                
                print(f'📊 Found {len(recent_repos)} repositories created in the last 2 hours:')
                for repo in recent_repos:
                    print(f'  📦 {repo[\"name\"]} - {repo[\"created_at\"]}')
                    
                if len(recent_repos) == 0:
                    print('📭 No new repositories found (this may be expected)')
            else:
                print(f'❌ Failed to fetch repositories: {repos_response.status_code}')
        else:
            print(f'❌ Failed to get user info: {user_response.status_code}')
        "
      continue-on-error: true

  # ============================================================================
  # STAGE 7: FINAL REPORTING & SUMMARY
  # ============================================================================
  final-summary:
    name: 📊 Pipeline Summary & Reporting
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning, code-generation, github-deployment]
    if: always()
    
    steps:
    - name: 📊 Generate Complete Pipeline Summary
      run: |
        echo "# 🔬 WATCHDOG Full Research Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 🎯 Pipeline Results" >> $GITHUB_STEP_SUMMARY
        echo "| Stage | Status | Output |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| 📚 Paper Scraping | ${{ needs.paper-scraping.result }} | ${{ needs.paper-scraping.outputs.papers_scraped || '0' }} papers |" >> $GITHUB_STEP_SUMMARY
        echo "| 📄 Text Extraction | ${{ needs.text-extraction.result }} | ${{ needs.text-extraction.outputs.chunks_created || '0' }} chunks |" >> $GITHUB_STEP_SUMMARY
        echo "| 🎯 Relevance Selection | ${{ needs.relevance-selection.result }} | ${{ needs.relevance-selection.outputs.relevant_papers || '0' }} relevant |" >> $GITHUB_STEP_SUMMARY
        echo "| 📋 Project Planning | ${{ needs.project-planning.result }} | ${{ needs.project-planning.outputs.plans_created || '0' }} plans |" >> $GITHUB_STEP_SUMMARY
        echo "| 👥 Code Generation | ${{ needs.code-generation.result }} | ${{ needs.code-generation.outputs.projects_created || '0' }} projects |" >> $GITHUB_STEP_SUMMARY
        echo "| 🚀 GitHub Deployment | ${{ needs.github-deployment.result }} | ${{ needs.github-deployment.outputs.repos_created || '0' }} repositories |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 📈 Research Metrics" >> $GITHUB_STEP_SUMMARY
        echo "- **Papers Scraped**: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Text Chunks**: ${{ needs.text-extraction.outputs.chunks_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Relevant Papers**: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Project Plans**: ${{ needs.project-planning.outputs.plans_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Generated Projects**: ${{ needs.code-generation.outputs.projects_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **GitHub Repositories**: ${{ needs.github-deployment.outputs.repos_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## 🔧 Pipeline Information" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run Number**: ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Max Papers Per Domain**: ${{ env.MAX_PAPERS_PER_DOMAIN }}" >> $GITHUB_STEP_SUMMARY
    
    - name: 🎉 Success Notification
      if: |
        needs.paper-scraping.result == 'success' && 
        needs.text-extraction.result == 'success' &&
        needs.relevance-selection.result == 'success' &&
        needs.project-planning.result == 'success' &&
        needs.code-generation.result == 'success'
      run: |
        echo "🎉 WATCHDOG RESEARCH PIPELINE SUCCESS!"
        echo "✅ Complete end-to-end research pipeline executed successfully"
        echo ""
        echo "📊 FINAL RESULTS:"
        echo "  📚 Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}"
        echo "  📄 Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}"
        echo "  🎯 Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}"
        echo "  📋 Project Plans: ${{ needs.project-planning.outputs.plans_created || '0' }}"
        echo "  👥 Generated Projects: ${{ needs.code-generation.outputs.projects_created || '0' }}"
        echo "  🚀 GitHub Repositories: ${{ needs.github-deployment.outputs.repos_created || '0' }}"
        echo ""
        echo "🔗 Check artifacts for detailed outputs from each stage"
    
    - name: ⚠️ Partial Success Notification
      if: |
        (needs.paper-scraping.result == 'success' || needs.paper-scraping.result == 'skipped') &&
        (needs.text-extraction.result == 'failure' || needs.relevance-selection.result == 'failure' || 
         needs.project-planning.result == 'failure' || needs.code-generation.result == 'failure')
      run: |
        echo "⚠️ WATCHDOG Pipeline completed with some issues"
        echo "✅ Initial stages completed successfully"
        echo "❌ Some downstream stages failed"
        echo "🔧 Check the failed jobs for details"
        echo ""
        echo "📊 PARTIAL RESULTS:"
        echo "  📚 Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}"
        echo "  📄 Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}"
        echo "  🎯 Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}"
    
    - name: ❌ Failure Notification
      if: needs.paper-scraping.result == 'failure'
      run: |
        echo "❌ WATCHDOG RESEARCH PIPELINE FAILED!"
        echo "🚨 Initial paper scraping failed - pipeline cannot continue"
        echo "🔧 Please check the scraping job logs and fix issues"
        echo "💡 Common issues: API rate limits, network connectivity, invalid domains"
        exit 1
    
    - name: 📧 Create Issue on Critical Failure
      if: failure() && github.ref == 'refs/heads/main'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `🚨 WATCHDOG Research Pipeline Failed - Run #${{ github.run_number }}`,
            body: `
            ## Research Pipeline Failure Report
            
            **Run**: #${{ github.run_number }}
            **Commit**: ${{ github.sha }}
            **Branch**: ${{ github.ref_name }}
            **Trigger**: ${{ github.event_name }}
            
            ### Stage Results
            - 📚 Paper Scraping: ${{ needs.paper-scraping.result }}
            - 📄 Text Extraction: ${{ needs.text-extraction.result }}
            - 🎯 Relevance Selection: ${{ needs.relevance-selection.result }}
            - 📋 Project Planning: ${{ needs.project-planning.result }}
            - 👥 Code Generation: ${{ needs.code-generation.result }}
            - 🚀 GitHub Deployment: ${{ needs.github-deployment.result }}
            
            ### Outputs
            - Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}
            - Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}
            - Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}
            - Project Plans: ${{ needs.project-planning.outputs.plans_created || '0' }}
            - Generated Projects: ${{ needs.code-generation.outputs.projects_created || '0' }}
            - GitHub Repositories: ${{ needs.github-deployment.outputs.repos_created || '0' }}
            
            **Action Required**: Please review the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) and fix the issues.
            `,
            labels: ['bug', 'research-pipeline', 'high-priority']
          })
      continue-on-error: true
