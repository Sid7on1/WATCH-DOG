name: ğŸ”¬ WATCHDOG Full Research Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Daily research pipeline at 1:20 AM India time (19:50 UTC)
    - cron: '50 19 * * *'
  workflow_dispatch:
    inputs:
      max_papers_per_domain:
        description: 'Maximum papers to scrape per domain'
        required: false
        default: '5'
        type: string
      skip_scraping:
        description: 'Skip paper scraping (use existing PDFs)'
        required: false
        default: false
        type: boolean
      skip_extraction:
        description: 'Skip PDF text extraction'
        required: false
        default: false
        type: boolean
      skip_selection:
        description: 'Skip relevance selection'
        required: false
        default: false
        type: boolean
      skip_planning:
        description: 'Skip project planning'
        required: false
        default: false
        type: boolean
      skip_coding:
        description: 'Skip code generation'
        required: false
        default: false
        type: boolean
      force_deploy:
        description: 'Force deployment to GitHub'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  MAX_PAPERS_PER_DOMAIN: ${{ github.event.inputs.max_papers_per_domain || '5' }}

jobs:
  # ============================================================================
  # STAGE 1: PAPER SCRAPING
  # ============================================================================
  paper-scraping:
    name: ğŸ“š ArXiv Paper Scraping
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_scraping != 'true'
    outputs:
      papers_scraped: ${{ steps.scrape-papers.outputs.papers_count }}
      scraping_success: ${{ steps.scrape-papers.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install WATCHDOG Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for scraping
          pip install requests python-dotenv lxml beautifulsoup4 arxiv urllib3
        fi
        echo "âœ… Dependencies installed for scraping stage"
    
    - name: ğŸ” Configure API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "ğŸ”‘ Configuring API keys for WATCHDOG system..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "GEMINI_API_KEY=$GEMINI_API_KEY" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "âœ… Environment configured with all API keys"
    
    - name: ğŸ“š Run ArXiv Paper Scraper
      id: scrape-papers
      run: |
        echo "ğŸ” Starting ArXiv paper scraping..."
        echo "ğŸ“Š Max papers per domain: ${{ env.MAX_PAPERS_PER_DOMAIN }}"
        
        python -c "
        import sys
        import traceback
        
        try:
            import scraper
            print('âœ… Scraper module loaded successfully')
            
            # Initialize scraper
            arxiv_scraper = scraper.ArxivScraper()
            print('âœ… ArXiv scraper initialized')
            
            # Run scraping with specified limits
            max_papers = int('${{ env.MAX_PAPERS_PER_DOMAIN }}')
            results = arxiv_scraper.scrape_all_domains(
                max_results_per_domain=max_papers,
                download_pdfs=True
            )
            
            # Count total papers
            total_papers = sum(len(papers) for papers in results.values())
            print(f'ğŸ“Š Total papers scraped: {total_papers}')
            
            # Save metadata and summary
            arxiv_scraper.save_consolidated_metadata(results)
            arxiv_scraper.generate_summary_report(results)
            
            # Complete workflow
            arxiv_scraper.github_manager.workflow_end()
            
            print(f'papers_count={total_papers}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('âœ… Paper scraping completed successfully!')
            
        except Exception as e:
            print(f'âŒ Scraping failed: {e}')
            traceback.print_exc()
            print('papers_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ“Š Upload Scraping Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: scraped-papers-${{ github.run_number }}
        path: |
          artifacts/pdfs/
          artifacts/metadata.txt
          artifacts/scraping_summary.txt
        retention-days: 7

  # ============================================================================
  # STAGE 2: PDF TEXT EXTRACTION
  # ============================================================================
  text-extraction:
    name: ğŸ“„ PDF Text Extraction
    runs-on: ubuntu-latest
    needs: paper-scraping
    if: |
      always() && 
      (needs.paper-scraping.result == 'success' || github.event.inputs.skip_scraping == 'true') &&
      github.event.inputs.skip_extraction != 'true'
    outputs:
      chunks_created: ${{ steps.extract-text.outputs.chunks_count }}
      extraction_success: ${{ steps.extract-text.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ Download Scraped Papers
      if: needs.paper-scraping.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: scraped-papers-${{ github.run_number }}
        path: ./
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install PDF Processing Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for PDF processing
          pip install PyPDF2 pdfplumber pathlib
        fi
        echo "âœ… Dependencies installed for PDF extraction stage"
    
    - name: ğŸ“„ Extract Text from PDFs
      id: extract-text
      run: |
        echo "ğŸ“„ Starting PDF text extraction..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import extractor
            print('âœ… Extractor module loaded successfully')
            
            # Initialize extractor
            pdf_extractor = extractor.PDFTextExtractor()
            print('âœ… PDF extractor initialized')
            
            # Process all PDFs
            pdf_extractor.process_all_pdfs()
            
            # Count created chunks
            import os
            from pathlib import Path
            
            texts_dir = Path('artifacts/pdf-txts')
            chunk_count = 0
            if texts_dir.exists():
                for pdf_dir in texts_dir.iterdir():
                    if pdf_dir.is_dir():
                        chunk_files = list(pdf_dir.glob('chunk_*.txt'))
                        chunk_count += len(chunk_files)
            
            print(f'ğŸ“Š Total text chunks created: {chunk_count}')
            print(f'chunks_count={chunk_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('âœ… Text extraction completed successfully!')
            
        except Exception as e:
            print(f'âŒ Text extraction failed: {e}')
            traceback.print_exc()
            print('chunks_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ“Š Upload Text Extraction Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: extracted-texts-${{ github.run_number }}
        path: |
          artifacts/pdf-txts/
        retention-days: 7

  # ============================================================================
  # STAGE 3: RELEVANCE SELECTION
  # ============================================================================
  relevance-selection:
    name: ğŸ¯ Intelligent Relevance Selection
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction]
    if: |
      always() && 
      (needs.text-extraction.result == 'success' || github.event.inputs.skip_extraction == 'true') &&
      github.event.inputs.skip_selection != 'true'
    outputs:
      relevant_papers: ${{ steps.select-relevant.outputs.relevant_count }}
      selection_success: ${{ steps.select-relevant.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ Download Extracted Texts
      if: needs.text-extraction.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: extracted-texts-${{ github.run_number }}
        path: ./
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Selection Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for AI selection
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "âœ… Dependencies installed for selection stage"
    
    - name: ğŸ” Configure AI API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "ğŸ”‘ Configuring AI API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "âœ… AI APIs configured"
    
    - name: ğŸ¯ Run Intelligent PDF Selector
      id: select-relevant
      run: |
        echo "ğŸ¯ Starting intelligent relevance selection..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import selector
            print('âœ… Selector module loaded successfully')
            
            # Initialize selector
            pdf_selector = selector.IntelligentPDFSelector()
            print('âœ… Intelligent PDF selector initialized')
            
            # Process all PDF texts for relevance
            pdf_selector.process_all_pdfs()
            
            # Count relevant papers
            import os
            from pathlib import Path
            
            relevant_dir = Path('artifacts/relevant')
            relevant_count = 0
            if relevant_dir.exists():
                relevant_count = len([d for d in relevant_dir.iterdir() if d.is_dir()])
            
            print(f'ğŸ“Š Relevant papers found: {relevant_count}')
            print(f'relevant_count={relevant_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('âœ… Relevance selection completed successfully!')
            
        except Exception as e:
            print(f'âŒ Relevance selection failed: {e}')
            traceback.print_exc()
            print('relevant_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ“Š Upload Selection Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: relevant-papers-${{ github.run_number }}
        path: |
          artifacts/relevant/
        retention-days: 7

  # ============================================================================
  # STAGE 4: PROJECT PLANNING
  # ============================================================================
  project-planning:
    name: ğŸ“‹ Intelligent Project Planning
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection]
    if: |
      always() && 
      (needs.relevance-selection.result == 'success' || github.event.inputs.skip_selection == 'true') &&
      github.event.inputs.skip_planning != 'true'
    outputs:
      plans_created: ${{ steps.create-plans.outputs.plans_count }}
      planning_success: ${{ steps.create-plans.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ Download Relevant Papers
      if: needs.relevance-selection.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: relevant-papers-${{ github.run_number }}
        path: ./
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Planning Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for planning
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "âœ… Dependencies installed for planning stage"
    
    - name: ğŸ” Configure Planning API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "ğŸ”‘ Configuring planning API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "âœ… Planning APIs configured"
    
    - name: ğŸ“‹ Run Intelligent Project Planner
      id: create-plans
      run: |
        echo "ğŸ“‹ Starting intelligent project planning..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import planner
            print('âœ… Planner module loaded successfully')
            
            # Initialize planner
            project_planner = planner.IntelligentProjectPlanner()
            print('âœ… Intelligent project planner initialized')
            
            # Process all relevant papers and create plans
            project_planner.process_all_relevant_papers()
            
            # Count created plans
            import os
            from pathlib import Path
            
            structures_dir = Path('artifacts/structures')
            plans_count = 0
            if structures_dir.exists():
                for paper_dir in structures_dir.iterdir():
                    if paper_dir.is_dir() and (paper_dir / 'plan.json').exists():
                        plans_count += 1
            
            print(f'ğŸ“Š Project plans created: {plans_count}')
            print(f'plans_count={plans_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('âœ… Project planning completed successfully!')
            
        except Exception as e:
            print(f'âŒ Project planning failed: {e}')
            traceback.print_exc()
            print('plans_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ“Š Upload Planning Artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: project-plans-${{ github.run_number }}
        path: |
          artifacts/structures/
        retention-days: 7

  # ============================================================================
  # STAGE 5: PROJECT MANAGEMENT & CODE GENERATION
  # ============================================================================
  code-generation:
    name: ğŸ‘¥ Multi-Agent Code Generation
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning]
    if: |
      always() && 
      (needs.project-planning.result == 'success' || github.event.inputs.skip_planning == 'true') &&
      github.event.inputs.skip_coding != 'true'
    outputs:
      projects_created: ${{ steps.generate-code.outputs.projects_count }}
      coding_success: ${{ steps.generate-code.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ Download Project Plans
      if: needs.project-planning.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: project-plans-${{ github.run_number }}
        path: ./
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Coding Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for coding (threading/queue are built-in)
          pip install requests python-dotenv google-generativeai cohere groq
        fi
        echo "âœ… Dependencies installed for coding stage"
    
    - name: ğŸ” Configure Coding API Keys
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
        OPEN_API: ${{ secrets.OPEN_API }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        COHERE_API: ${{ secrets.COHERE_API }}
        GROQ_API: ${{ secrets.GROQ_API }}
        HF_API: ${{ secrets.HF_API }}
      run: |
        echo "ğŸ”‘ Configuring coding API keys..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "OPEN_API=$OPEN_API" >> .env
        echo "gemini_API=$GEMINI_API_KEY" >> .env
        echo "cohere_API=$COHERE_API" >> .env
        echo "groq_API=$GROQ_API" >> .env
        echo "HF_API=$HF_API" >> .env
        echo "âœ… Coding APIs configured"
    
    - name: ğŸ‘¥ Run Multi-Agent Project Manager
      id: generate-code
      run: |
        echo "ğŸ‘¥ Starting multi-agent code generation..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import manager
            print('âœ… Manager module loaded successfully')
            
            # Initialize project manager
            project_manager = manager.ProjectManager()
            print('âœ… Multi-agent project manager initialized')
            
            # Process all project plans and generate code
            project_manager.process_all_project_plans()
            
            # Count created projects
            import os
            from pathlib import Path
            
            projects_dir = Path('artifacts/projects')
            projects_count = 0
            if projects_dir.exists():
                projects_count = len([d for d in projects_dir.iterdir() if d.is_dir()])
            
            print(f'ğŸ“Š Projects created: {projects_count}')
            print(f'projects_count={projects_count}' >> '$GITHUB_OUTPUT')
            print('success=true' >> '$GITHUB_OUTPUT')
            print('âœ… Code generation completed successfully!')
            
        except Exception as e:
            print(f'âŒ Code generation failed: {e}')
            traceback.print_exc()
            print('projects_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ“Š Upload Generated Projects
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: generated-projects-${{ github.run_number }}
        path: |
          artifacts/projects/
        retention-days: 30

  # ============================================================================
  # STAGE 6: GITHUB DEPLOYMENT
  # ============================================================================
  github-deployment:
    name: ğŸš€ GitHub Repository Deployment
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning, code-generation]
    if: |
      always() && 
      (needs.code-generation.result == 'success' || github.event.inputs.skip_coding == 'true') &&
      github.event.inputs.force_deploy == 'true'
    outputs:
      repos_created: ${{ steps.deploy-repos.outputs.repos_count }}
      deployment_success: ${{ steps.deploy-repos.outputs.success }}
    
    steps:
    - name: ğŸ”„ Checkout Code
      uses: actions/checkout@v4
    
    - name: ğŸ“¥ Download Generated Projects
      if: needs.code-generation.result == 'success'
      uses: actions/download-artifact@v3
      with:
        name: generated-projects-${{ github.run_number }}
        path: ./
    
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: ğŸ“¦ Install Deployment Dependencies
      run: |
        python -m pip install --upgrade pip
        # Install comprehensive requirements
        if [ -f requirements.txt ]; then 
          pip install -r requirements.txt
        else
          # Fallback essential dependencies for deployment (base64/shutil/tempfile are built-in)
          pip install requests python-dotenv
        fi
        echo "âœ… Dependencies installed for deployment stage"
    
    - name: ğŸ” Configure GitHub API
      env:
        API_GITHUB: ${{ secrets.API_GITHUB }}
      run: |
        echo "ğŸ”‘ Configuring GitHub API..."
        echo "GITHUB_PAT=$API_GITHUB" >> .env
        echo "GITHUB_API=$API_GITHUB" >> .env
        echo "âœ… GitHub API configured"
    
    - name: ğŸš€ Deploy Projects to GitHub Repositories
      id: deploy-repos
      run: |
        echo "ğŸš€ Starting GitHub repository deployment..."
        
        python -c "
        import sys
        import traceback
        
        try:
            import pusher
            print('âœ… Pusher module loaded successfully')
            
            # Initialize GitHub repository manager
            github_manager = pusher.GitHubRepositoryManager()
            print('âœ… GitHub repository manager initialized')
            
            # Print current status
            github_manager.print_status()
            
            # Process all projects and create repositories
            success = github_manager.process_all_projects()
            
            if success:
                print('âœ… All projects deployed to GitHub successfully!')
                
                # Get repository count
                from pathlib import Path
                projects_dir = Path('artifacts/projects')
                repos_count = 0
                if projects_dir.exists():
                    repos_count = len([d for d in projects_dir.iterdir() if d.is_dir()])
                
                print(f'ğŸ“Š Repositories created: {repos_count}')
                print(f'repos_count={repos_count}' >> '$GITHUB_OUTPUT')
                print('success=true' >> '$GITHUB_OUTPUT')
                
                # Final status report
                print('\\nğŸ“Š Final deployment status:')
                github_manager.print_status()
            else:
                print('âŒ Some projects failed to deploy')
                print('repos_count=0' >> '$GITHUB_OUTPUT')
                print('success=false' >> '$GITHUB_OUTPUT')
                sys.exit(1)
                
        except Exception as e:
            print(f'âŒ GitHub deployment failed: {e}')
            traceback.print_exc()
            print('repos_count=0' >> '$GITHUB_OUTPUT')
            print('success=false' >> '$GITHUB_OUTPUT')
            sys.exit(1)
        "
    
    - name: ğŸ” Verify Repository Deployments
      env:
        GITHUB_API: ${{ secrets.API_GITHUB }}
      run: |
        echo "ğŸ” Verifying deployed repositories..."
        
        python -c "
        import requests
        import os
        from datetime import datetime, timedelta
        
        github_token = os.getenv('GITHUB_API')
        headers = {
            'Authorization': f'token {github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # Get user info
        user_response = requests.get('https://api.github.com/user', headers=headers)
        if user_response.status_code == 200:
            username = user_response.json()['login']
            print(f'ğŸ‘¤ Verifying repositories for: {username}')
            
            # Get recent repositories
            repos_response = requests.get(
                f'https://api.github.com/users/{username}/repos?sort=created&direction=desc&per_page=20',
                headers=headers
            )
            
            if repos_response.status_code == 200:
                repos = repos_response.json()
                recent_repos = []
                
                # Filter repositories created in the last 2 hours
                cutoff_time = datetime.now() - timedelta(hours=2)
                
                for repo in repos:
                    created_at = datetime.fromisoformat(repo['created_at'].replace('Z', '+00:00'))
                    if created_at.replace(tzinfo=None) >= cutoff_time:
                        recent_repos.append(repo)
                
                print(f'ğŸ“Š Found {len(recent_repos)} repositories created in the last 2 hours:')
                for repo in recent_repos:
                    print(f'  ğŸ“¦ {repo[\"name\"]} - {repo[\"created_at\"]}')
                    
                if len(recent_repos) == 0:
                    print('ğŸ“­ No new repositories found (this may be expected)')
            else:
                print(f'âŒ Failed to fetch repositories: {repos_response.status_code}')
        else:
            print(f'âŒ Failed to get user info: {user_response.status_code}')
        "
      continue-on-error: true

  # ============================================================================
  # STAGE 7: FINAL REPORTING & SUMMARY
  # ============================================================================
  final-summary:
    name: ğŸ“Š Pipeline Summary & Reporting
    runs-on: ubuntu-latest
    needs: [paper-scraping, text-extraction, relevance-selection, project-planning, code-generation, github-deployment]
    if: always()
    
    steps:
    - name: ğŸ“Š Generate Complete Pipeline Summary
      run: |
        echo "# ğŸ”¬ WATCHDOG Full Research Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ¯ Pipeline Results" >> $GITHUB_STEP_SUMMARY
        echo "| Stage | Status | Output |" >> $GITHUB_STEP_SUMMARY
        echo "|-------|--------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ“š Paper Scraping | ${{ needs.paper-scraping.result }} | ${{ needs.paper-scraping.outputs.papers_scraped || '0' }} papers |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ“„ Text Extraction | ${{ needs.text-extraction.result }} | ${{ needs.text-extraction.outputs.chunks_created || '0' }} chunks |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ¯ Relevance Selection | ${{ needs.relevance-selection.result }} | ${{ needs.relevance-selection.outputs.relevant_papers || '0' }} relevant |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ“‹ Project Planning | ${{ needs.project-planning.result }} | ${{ needs.project-planning.outputs.plans_created || '0' }} plans |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸ‘¥ Code Generation | ${{ needs.code-generation.result }} | ${{ needs.code-generation.outputs.projects_created || '0' }} projects |" >> $GITHUB_STEP_SUMMARY
        echo "| ğŸš€ GitHub Deployment | ${{ needs.github-deployment.result }} | ${{ needs.github-deployment.outputs.repos_created || '0' }} repositories |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ“ˆ Research Metrics" >> $GITHUB_STEP_SUMMARY
        echo "- **Papers Scraped**: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Text Chunks**: ${{ needs.text-extraction.outputs.chunks_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Relevant Papers**: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Project Plans**: ${{ needs.project-planning.outputs.plans_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Generated Projects**: ${{ needs.code-generation.outputs.projects_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "- **GitHub Repositories**: ${{ needs.github-deployment.outputs.repos_created || '0' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ”§ Pipeline Information" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Run Number**: ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Max Papers Per Domain**: ${{ env.MAX_PAPERS_PER_DOMAIN }}" >> $GITHUB_STEP_SUMMARY
    
    - name: ğŸ‰ Success Notification
      if: |
        needs.paper-scraping.result == 'success' && 
        needs.text-extraction.result == 'success' &&
        needs.relevance-selection.result == 'success' &&
        needs.project-planning.result == 'success' &&
        needs.code-generation.result == 'success'
      run: |
        echo "ğŸ‰ WATCHDOG RESEARCH PIPELINE SUCCESS!"
        echo "âœ… Complete end-to-end research pipeline executed successfully"
        echo ""
        echo "ğŸ“Š FINAL RESULTS:"
        echo "  ğŸ“š Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}"
        echo "  ğŸ“„ Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}"
        echo "  ğŸ¯ Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}"
        echo "  ğŸ“‹ Project Plans: ${{ needs.project-planning.outputs.plans_created || '0' }}"
        echo "  ğŸ‘¥ Generated Projects: ${{ needs.code-generation.outputs.projects_created || '0' }}"
        echo "  ğŸš€ GitHub Repositories: ${{ needs.github-deployment.outputs.repos_created || '0' }}"
        echo ""
        echo "ğŸ”— Check artifacts for detailed outputs from each stage"
    
    - name: âš ï¸ Partial Success Notification
      if: |
        (needs.paper-scraping.result == 'success' || needs.paper-scraping.result == 'skipped') &&
        (needs.text-extraction.result == 'failure' || needs.relevance-selection.result == 'failure' || 
         needs.project-planning.result == 'failure' || needs.code-generation.result == 'failure')
      run: |
        echo "âš ï¸ WATCHDOG Pipeline completed with some issues"
        echo "âœ… Initial stages completed successfully"
        echo "âŒ Some downstream stages failed"
        echo "ğŸ”§ Check the failed jobs for details"
        echo ""
        echo "ğŸ“Š PARTIAL RESULTS:"
        echo "  ğŸ“š Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}"
        echo "  ğŸ“„ Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}"
        echo "  ğŸ¯ Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}"
    
    - name: âŒ Failure Notification
      if: needs.paper-scraping.result == 'failure'
      run: |
        echo "âŒ WATCHDOG RESEARCH PIPELINE FAILED!"
        echo "ğŸš¨ Initial paper scraping failed - pipeline cannot continue"
        echo "ğŸ”§ Please check the scraping job logs and fix issues"
        echo "ğŸ’¡ Common issues: API rate limits, network connectivity, invalid domains"
        exit 1
    
    - name: ğŸ“§ Create Issue on Critical Failure
      if: failure() && github.ref == 'refs/heads/main'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `ğŸš¨ WATCHDOG Research Pipeline Failed - Run #${{ github.run_number }}`,
            body: `
            ## Research Pipeline Failure Report
            
            **Run**: #${{ github.run_number }}
            **Commit**: ${{ github.sha }}
            **Branch**: ${{ github.ref_name }}
            **Trigger**: ${{ github.event_name }}
            
            ### Stage Results
            - ğŸ“š Paper Scraping: ${{ needs.paper-scraping.result }}
            - ğŸ“„ Text Extraction: ${{ needs.text-extraction.result }}
            - ğŸ¯ Relevance Selection: ${{ needs.relevance-selection.result }}
            - ğŸ“‹ Project Planning: ${{ needs.project-planning.result }}
            - ğŸ‘¥ Code Generation: ${{ needs.code-generation.result }}
            - ğŸš€ GitHub Deployment: ${{ needs.github-deployment.result }}
            
            ### Outputs
            - Papers Scraped: ${{ needs.paper-scraping.outputs.papers_scraped || '0' }}
            - Text Chunks: ${{ needs.text-extraction.outputs.chunks_created || '0' }}
            - Relevant Papers: ${{ needs.relevance-selection.outputs.relevant_papers || '0' }}
            - Project Plans: ${{ needs.project-planning.outputs.plans_created || '0' }}
            - Generated Projects: ${{ needs.code-generation.outputs.projects_created || '0' }}
            - GitHub Repositories: ${{ needs.github-deployment.outputs.repos_created || '0' }}
            
            **Action Required**: Please review the [workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}) and fix the issues.
            `,
            labels: ['bug', 'research-pipeline', 'high-priority']
          })
      continue-on-error: true
